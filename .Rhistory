fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Visualize data and training/testing regions
bikes %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
test <- bikes %>%
filter(date >= ymd("2012-07-01"))
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Model using the augmented features
fit_lm <- lm(cnt ~ ., data = train_augmented)
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
# RMSE
sqrt(mean(fit_lm$residuals^2))
test_augmented <- test %>%
tk_augment_timeseries_signature()
test_augmented
yhat_test <- predict(fit_lm, newdata = test_augmented)
pred_test <- test %>%
add_column(yhat = yhat_test) %>%
mutate(.resid = cnt - yhat)
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
train
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
summary(fit_lm)
test_augmented
setwd("~/1. MIT Courses/2020-Spring/Analytics Edge/Lecture files/L10")
library(RColorBrewer)
library(ggplot2)
library(flexclust)
library(caret)
library(cluster)
library(dplyr)
simple <- read.csv("simple.csv")
names(simple) <- c("x", "y")
# Data import
airline <- read.csv("AirlinesCluster.csv")
# Data display
head(airline, 16)
tail(airline, 5)
colMeans(airline)
apply(airline, 2, sd)
# Normalization
pp <- preProcess(airline, method=c("center", "scale"))
airline.scaled <- predict(pp, airline)
# Display of nromalized data
head(round(airline.scaled, 2), 16)
tail(round(airline.scaled, 2), 5)
set.seed(144)
# Running the k means algorithm
mod <- kmeans(airline.scaled, iter.max=100, 8)
cluster.assignment.kmeans <- mod$cluster
# Summary of results
t(round(mod$centers, 2))
round(t(mod$centers) * pp$std + pp$mean)
table(mod$cluster)
# Running the k means algorithm for each k
dat <- data.frame(k = 1:100)
dat$SS <- sapply(dat$k, function(k) {
set.seed(144)
kmeans(airline.scaled, iter.max=100, k)$tot.withinss
})
# Plotting the results
print(ggplot(dat, aes(x=k, y=SS)) +
geom_line(lwd=2) +
theme_bw() +
xlab("Number of Clusters (k)") +
ylab("Within-Cluster Sum of Squares") +
ylim(0, 25000) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18)))
d <- dist(airline.scaled)
mod.hclust <- hclust(d, method="ward.D2")
dissimilarity.all <- c(75,50,32,25,20,15)
plot(mod.hclust, labels=F, xlab=NA, ylab="Dissimilarity", sub=NA, main=NA)
plot(mod.hclust, labels=F, xlab=NA, ylab="Dissimilarity", sub=NA, main=NA)
# Varying the number of clusters
dat.hc.airline <- data.frame(nclust = seq_along(mod.hclust$height),
dissimilarity = rev(mod.hclust$height))
# Plotting the results
print(ggplot(dat.hc.airline, aes(x=nclust, y=dissimilarity)) +
geom_line(lwd=2) +
theme_bw() +
xlab("Number of Clusters") +
ylab("Dissimilarity") +
xlim(0, 100) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18)))
# Assignment of data points to clusters
assignments <- cutree(mod.hclust, 7)
# Display clusters
round(sapply(split(airline.scaled, assignments), colMeans), 2)
table(assignments)
# Data import
data(auto)
simple <- read.csv("simple.csv")
names(simple) <- c("x", "y")
names(simple) <- c("x", "y")
# Baseline scatter plot
print(ggplot(simple, aes(x=x, y=y)) +
geom_point(size=3) +
theme_bw() +
xlim(0,70) + ylim(0,100) +
xlab("Time Since Enrolled (months)") +
ylab("Balance (1,000 miles)") +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
-
# Scatter plot (tenure rescaled)
print(ggplot(simple, aes(x=x/12, y=y)) +
geom_point(size=3) +
theme_bw() +
xlim(0,70) + ylim(0,100) +
xlab("Time Since Enrolled (years)") +
ylab("Balance (1,000 miles)") +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
print(ggplot(simple, aes(x=x, y=y/10)) +
geom_point(size=3) +
theme_bw() +
xlim(0,70) + ylim(0,100) +
xlab("Time Since Enrolled (months)") +
ylab("Balance (10,000 miles)") +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
# Normalization
simple.norm <- data.frame(scale(simple))
print(ggplot(simple.norm, aes(x=x, y=y)) +
geom_point(size=3) +
theme_bw() +
xlim(-2,2) + ylim(-2,2) +
xlab("Time Since Enrolled (normalized)") +
ylab("Balance (normalized)") +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
# Randomly Select Centroid Locations
centroid1 <- data.frame(x=c(0.1, 0.75, 0.25), y=c(0.5, 0.6, -1))
# Data points are unassigned at that point
plot1 <- data.frame(x=c(simple.norm$x, centroid1$x),
y=c(simple.norm$y, centroid1$y),
num=factor(rep(c(4, 1, 2, 3), c(20, 1, 1, 1))),
type=factor(rep(c(1, 2), c(20, 3))))
print(ggplot(plot1, aes(x=x, y=y, col=num, pch=type)) +
geom_point(size=3) +
theme_bw() +
xlim(-2,2) + ylim(-2,2) +
xlab("Time Since Enrolled (Normalized)") +
ylab("Balance (Normalized)") +
scale_color_manual(values=c("1"="red", "2"="blue", "3"="black", "4"="darkgray")) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
# Assignment of data points to closest centroid
d1 <- outer(1:nrow(simple.norm), 1:3, function(i, j) {
sqrt((simple.norm[i,1] - centroid1[j,1])^2 + (simple.norm[i,2] - centroid1[j,2])^2)
})
assigned1 <- apply(d1, 1, which.min)
plot1$num <- factor(c(assigned1, 1:3))
print(ggplot(plot1, aes(x=x, y=y, col=num, pch=type)) +
geom_point(size=3) +
theme_bw() +
xlim(-2,2) + ylim(-2,2) +
xlab("Time Since Enrolled (Normalized)") +
ylab("Balance (Normalized)") +
scale_color_manual(values=c("1"="red", "2"="blue", "3"="black")) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
# Update of centroids
centroid2 <- rbind(colMeans(simple.norm[assigned1 == 1,]),
colMeans(simple.norm[assigned1 == 2,]),
colMeans(simple.norm[assigned1 == 3,]))
plot2 <- plot1
plot2[21:23,1:2] <- centroid2
print(ggplot(plot2, aes(x=x, y=y, col=num, pch=type)) +
geom_point(size=3) +
theme_bw() +
xlim(-2,2) + ylim(-2,2) +
xlab("Time Since Enrolled (Normalized)") +
ylab("Balance (Normalized)") +
scale_color_manual(values=c("1"="red", "2"="blue", "3"="black")) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
# Assignment of data points to closest centroid
d2 <- outer(1:nrow(simple.norm), 1:3, function(i, j) {
sqrt((simple.norm[i,1] - centroid2[j,1])^2 + (simple.norm[i,2] - centroid2[j,2])^2)
})
assigned2 <- apply(d2, 1, which.min)
plot2$num <- factor(c(assigned2, 1:3))
print(ggplot(plot2, aes(x=x, y=y, col=num, pch=type)) +
geom_point(size=3) +
theme_bw() +
xlim(-2,2) + ylim(-2,2) +
xlab("Time Since Enrolled (Normalized)") +
ylab("Balance (Normalized)") +
scale_color_manual(values=c("1"="red", "2"="blue", "3"="black")) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
# Update of centroids
centroid3 <- rbind(colMeans(simple.norm[assigned2 == 1,]),
colMeans(simple.norm[assigned2 == 2,]),
colMeans(simple.norm[assigned2 == 3,]))
plot3 <- plot2
plot3[21:23,1:2] <- centroid3
print(ggplot(plot3, aes(x=x, y=y, col=num, pch=type)) +
geom_point(size=3) +
theme_bw() +
xlim(-2,2) + ylim(-2,2) +
xlab("Time Since Enrolled (Normalized)") +
ylab("Balance (Normalized)") +
scale_color_manual(values=c("1"="red", "2"="blue", "3"="black")) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
# Assignment of data points to closest centroid
d3 <- outer(1:nrow(simple.norm), 1:3, function(i, j) {
sqrt((simple.norm[i,1] - centroid3[j,1])^2 + (simple.norm[i,2] - centroid3[j,2])^2)
})
assigned3 <- apply(d3, 1, which.min)
table(assigned3 == assigned2)
plot3$num <- factor(c(assigned3, 1:3))
print(ggplot(plot3, aes(x=x, y=y, col=num, pch=type)) +
geom_point(size=3) +
theme_bw() +
xlim(-2,2) + ylim(-2,2) +
xlab("Time Since Enrolled (Normalized)") +
ylab("Balance (Normalized)") +
scale_color_manual(values=c("1"="red", "2"="blue", "3"="black")) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
###########Deficiencies in kMeans; introducing GMMs
#k-Means and extreme multi-collinearity
set.seed(2022)
x = c(rnorm(500), rnorm(500, 6))
y = c(rnorm(500), rnorm(500, 1))
newdat = data.frame(x, y)
print(ggplot(newdat, aes(x=x, y=y)) +
geom_point(size=3) +
theme_bw() +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
modnew <- kmeans(newdat, iter.max=100, 2)
newdatplot= data.frame(x, y, clus = factor(modnew$cluster))
print(ggplot(newdatplot, aes(x=x, y=y, col=clus)) +
geom_point(size=3) +
theme_bw() +
scale_color_manual(values=c("1"="orange", "2"="blue")) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
for(i in 1:30)
{
ymat = matrix(y, nrow=1000, ncol = i, byrow = F)
set.seed(1562)
newdat = data.frame(x, ymat)
modnew <- kmeans(newdat, iter.max=100, 2)
clustemp = modnew$cluster
clus = clustemp
if(mean(x[clus==1]) < mean(x[clus==2]))
{
clus[clustemp==1] = 2
clus[clustemp==2] = 1
}
colvec = ifelse(clus==1, "blue", "orange")
par(mgp = c(1.8, .5, 0), mar = c(3,3,1.5,1))
plot(x, y, pch = 16, col =colvec, main = paste("y repeated", i, "times", sep = " "))
Sys.sleep(1)
}
ymat = matrix(y, nrow=1000, ncol = 30, byrow = F)
set.seed(1562)
newdat = data.frame(x, ymat)
modnew <- kmeans(newdat, iter.max=100, 2)
newdatplot= data.frame(x, y, clus = factor(modnew$cluster))
print(ggplot(newdatplot, aes(x=x, y=y, col=clus)) +
geom_point(size=3) +
theme_bw() +
scale_color_manual(values=c("1"="orange", "2"="blue")) +
theme(axis.title=element_text(size=18), axis.text=element_text(size=18), legend.position="none"))
setwd("~/Google Drive/Colab Notebooks/R/5.2.hitter")
install.packages("ggcorrplot")
library(ggcorrplot)
install.packages(("caret"))
library(caret)
hitters_raw <- read.csv("Hitters.csv")
hitters_num <-hitters_raw[,2:18]
install.packages(("caret"))
ggcorrplot(round(cor(hitters_num),1), method="circle",
tl.col = "black",tl.srt = 45, tl.cex = 15, lab=TRUE)
round(cor(hitters_num),1)
lm.mod <- lm(Salary ~., data = hitters_num)
summary(lm.mod)
#Plot Salary vs. AtBat
plot(hitters_num$AtBat,hitters_num$Salary,
xlab='Number of times at bat in the season',
ylab='Annual salary ($ in thousands)',cex.lab=1.1)
abline(lm(hitters_num$Salary ~ hitters_num$AtBat), col="blue")
#Plot Salary vs. Walks
plot(hitters_num$Walks,hitters_num$Salary,
xlab='Number of walks in the career',
ylab='Annual salary ($ in thousands)',cex.lab=1.1)
abline(lm(hitters_num$Salary ~ hitters_num$Walks), col="blue")
#Plot Salary vs. Putouts
plot(hitters_num$PutOuts,hitters_num$Salary,
xlab='Number of hits in the season',
ylab='Annual salary ($ in thousands)',cex.lab=1.1)
abline(lm(hitters_num$Salary ~ hitters_num$PutOuts), col="blue")
#Normalize and split data
pp <- preProcess(hitters_raw, method=c("center", "scale"))
?preProcess
??preProcess
library(caret)
#Normalize and split data
pp <- preProcess(hitters_raw, method=c("center", "scale"))
Hitters <- predict(pp, hitters_raw)
set.seed(15071)
train.obs <- sort(sample(seq_len(nrow(Hitters)), 0.7*nrow(Hitters)))
train <- Hitters[train.obs,2:21]
test <- Hitters[-train.obs,2:21]
#Fit a Linear Regression and predict the test set
lin.mod <- lm(train$Salary ~ ., data = train)
pred.train = predict(lin.mod, newdata = train)
summary(lin.mod)
#Out-of-Sample R-squared
pred.test = predict(lin.mod, newdata = test)
SSE.test = sum((pred.test - test$Salary)^2)
SST.test = sum((test$Salary - mean(train$Salary))^2)
OSR2 = 1 - SSE.test/SST.test
OSR2
#Fit a restricted Linear Regression with variables with significance
head(train)
lin.mod2 <- lm(Salary ~ CRBI+AtBat+Hits+Walks+CRuns+CWalks+PutOuts,
data = train)
pred.train = predict(lin.mod2, newdata = train)
summary(lin.mod2)
#Out-of-Sample R-squared
pred.test = predict(lin.mod2, newdata = test)
SSE.test = sum((pred.test - test$Salary)^2)
SST.test = sum((test$Salary - mean(train$Salary))^2)
OSR2 = 1 - SSE.test/SST.test
OSR2
#c) Regularization
##### Ridge Regression
### Run Ridge Regression in the train Set
x.train=model.matrix(Salary~.-1,data=train)
y.train=train$Salary
x.test=model.matrix(Salary~.-1,data=test)
y.test=test$Salary
all.lambdas <- c(exp(seq(15, -10, -.1)))
cv.ridge=cv.glmnet(x.train,y.train,alpha=0,lambda=all.lambdas, nfold=10)
cv.lasso=cv.glmnet(x.train,y.train,alpha=1,lambda=all.lambdas, nfold=10)
library(ggcorrplot)
cv.ridge=cv.glmnet(x.train,y.train,alpha=0,lambda=all.lambdas, nfold=10)
?glmnet
??glmnet
install.packages('glmnet')
library(glmnet)
cv.ridge=cv.glmnet(x.train,y.train,alpha=0,lambda=all.lambdas, nfold=10)
cv.lasso=cv.glmnet(x.train,y.train,alpha=1,lambda=all.lambdas, nfold=10)
plot(cv.ridge)
plot(cv.lasso)
cv.ridge$lambda.min
cv.lasso$lambda.min
### Prediction on the train and test sets
# Re-train ridge regression and LASSO models on full training set.
ridge.final <- glmnet(x.train, y.train, alpha=0, lambda=cv.ridge$lambda.min)
lasso.final <- glmnet(x.train, y.train, alpha=1, lambda=cv.lasso$lambda.min)
ridge.final$beta
lasso.final$beta
pred.train.ridge <- predict(ridge.final, x.train)
pred.train.lasso <- predict(lasso.final, x.train)
R2.ridge <- 1-sum((pred.train.ridge-train$Salary)^2)/sum((mean(train$Salary)-train$Salary)^2)
R2.ridge
R2.lasso <- 1-sum((pred.train.lasso-train$Salary)^2)/sum((mean(train$Salary)-train$Salary)^2)
R2.lasso
pred.test.ridge <- predict(ridge.final, x.test)
pred.test.lasso <- predict(lasso.final, x.test)
# install.packages("lars")
# install.packages("xgboost")
# install.packages("glmnet")
library(caret)
library(lars)
library(glmnet)
library(xgboost)
library(plyr)
# We first read the diabetes dataset (from the lars package). This replaces the usual read.csv() command.
data(diabetes)
diabetes <- as.data.frame(cbind(y=diabetes$y, diabetes$x2))
head(diabetes)
# We split the data into a training and test set.
set.seed(1)
split = createDataPartition(diabetes$y, p = 0.7, list = FALSE)
diabetes.train = diabetes[split,]
diabetes.test = diabetes[-split,]
# To start, let's run a vanilla linear regression to use as a benchmark for the other methods we will learn to train today.
diabetes.model.lin <- lm(y ~ ., data=diabetes.train)
summary(diabetes.model.lin)
# The out-of-sample R2 is 0.368:
diabetes.preds.lin <- predict(diabetes.model.lin, newdata = diabetes.test)
diabetes.osr.lin <- 1 - sum((diabetes.preds.lin - diabetes.test$y)^2)/sum((mean(diabetes.train$y) - diabetes.test$y)^2)
# We'll first attempt to improve on the linear regression by using l1 regularization, also known as lasso.
# As you may have noticed, the train() function from the caret package provides a unified interface to access various prediction algorithms.
# We have used it for CART and random forests, and we will now use it for lasso.
# We provide the code first before explaining it.
diabetes.grid.l1 <- expand.grid(alpha = 1, lambda = 10^(-5:5)) # run expand.grid(alpha = c(0, 1), lambda = 10^(-5:5)) to understand what this line does
set.seed(1)
diabetes.cv.l1 <- train(y = diabetes.train$y,
x = data.matrix(subset(diabetes.train, select=-c(y))),
method = "glmnet",
trControl = trainControl(method="cv", number=5),
tuneGrid = diabetes.grid.l1)
# We view the performance metrics by:
diabetes.cv.l1$results
# The values of RSquared are missing for lambda >= 100 (there was also a warning about this after running train).
# This just means there was computational difficulty for these values of lambda.
# The best parameters by RMSE, or equivalently R2, are
diabetes.cv.l1$bestTune
# We can do another round of cross-validation by generating a finer parameter grid centered around on the best lambda.
diabetes.grid.l1 <- expand.grid(alpha = 1,
lambda = 10^(seq(-1, 1, by = 0.2)))
set.seed(1)
diabetes.cv.l1 <- train(y = diabetes.train$y,
x = data.matrix(subset(diabetes.train, select=-c(y))),
method = "glmnet",
trControl = trainControl(method="cv", number=5),
tuneGrid = diabetes.grid.l1)
diabetes.cv.l1$results
diabetes.cv.l1$bestTune
# We choose the best lambda and calculate the out-of-sample R2 as 0.508:
diabetes.model.l1 <- diabetes.cv.l1$finalModel
diabetes.preds.l1 <- predict(diabetes.model.l1, newx = data.matrix(subset(diabetes.test, select=-c(y))), s=diabetes.model.l1$lambdaOpt)
diabetes.osr.l1 <- 1 - sum((diabetes.preds.l1 - diabetes.test$y)^2)/sum((mean(diabetes.train$y) - diabetes.test$y)^2)
# We can also visualize how the cross-validated R2 varies with lambda.
ggplot(data = diabetes.cv.l1$results, aes(x = lambda, y = Rsquared)) + geom_point() + scale_x_log10()
# For reference, to train a lasso model without using the train function or cross-validation, the basic command is
glmnet(x = data.matrix(subset(diabetes.train, select=-c(y))), y = diabetes.train$y, alpha = 1, lambda = 3.981072)
# If we follow the same procedure as the previous section, but replacing alpha = 1 by alpha = 0, we are doing ridge regression instead of lasso.
# (If we choose a value of alpha between 0 and 1, we would be doing both l1 and l2 regularization, combining lasso and ridge regression.)
diabetes.grid.l2 <- expand.grid(alpha = 0,
lambda = 10^(-5:5))
set.seed(1)
diabetes.cv.l2 <- train(y = diabetes.train$y,
x = data.matrix(subset(diabetes.train, select=-c(y))),
method = "glmnet",
trControl = trainControl(method="cv", number=5),
tuneGrid = diabetes.grid.l2)
diabetes.cv.l2$results
diabetes.cv.l2$bestTune
# We generate a finer parameter grid around the best lambda:
diabetes.grid.l2 <- expand.grid(alpha = 0,
lambda = 10^(seq(0, 2, by = 0.2)))
set.seed(1)
diabetes.cv.l2 <- train(y = diabetes.train$y,
x = data.matrix(subset(diabetes.train, select=-c(y))),
method = "glmnet",
trControl = trainControl(method="cv", number=5),
tuneGrid = diabetes.grid.l2)
diabetes.cv.l2$results
diabetes.cv.l2$bestTune
# The out-of-sample R2 is 0.472:
diabetes.model.l2 <- diabetes.cv.l2$finalModel
diabetes.preds.l2 <- predict(diabetes.model.l2, newx = data.matrix(subset(diabetes.test, select=-c(y))), s=diabetes.model.l2$lambdaOpt)
diabetes.osr.l2 <- 1 - sum((diabetes.preds.l2 - diabetes.test$y)^2)/sum((mean(diabetes.train$y) - diabetes.test$y)^2)
# For reference, to train a ridge regression model without using the train function or cross-validation, the basic command is
glmnet(x = data.matrix(subset(diabetes.train, select=-c(y))), y = diabetes.train$y, alpha = 0, lambda = 39.81072)
# Using the glmnet function also makes it easier to access the coefficients.
# The coefficients for lasso are:
beta.lasso <- glmnet(x = data.matrix(subset(diabetes.train, select=-c(y))), y = diabetes.train$y, alpha = 1, lambda = diabetes.cv.l1$bestTune$lambda)$beta
head(beta.lasso)
# The coefficients for ridge regression are:
beta.ridge <- glmnet(x = data.matrix(subset(diabetes.train, select=-c(y))), y = diabetes.train$y, alpha = 0, lambda = diabetes.cv.l1$bestTune$lambda)$beta
head(beta.ridge)
# 11 coefficients are non-zero for lasso while all 64 are non-zero for ridge regression.
sum(beta.lasso != 0)
sum(beta.ridge != 0)
# For our boosting example we will use XGBoost. We can also do this with the train() function from caret.
# If we don't provide a set of parameters to cross-validate over, the method will select one automatically. (This applies to glmnet as well.)
# This will take a few minutes to run.
set.seed(1)
diabetes.cv.xgb <- train(y = diabetes.train$y,
x = data.matrix(subset(diabetes.train, select=-c(y))),
method = "xgbTree",
trControl = trainControl(method="cv", number=5))
# The out-of-sample R2 is 0.499:
diabetes.model.xgb <- diabetes.cv.xgb$finalModel
